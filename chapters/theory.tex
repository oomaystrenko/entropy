\section*{Что это такое?}~\
\\ 

Вспомним всем известную игру <<данетки>>. Так, чтобы понять, о чем идет речь, мы задаем уточняющие вопросы. Так вот, среднее число вопросов, необходимое, чтобы выяснить полную информацию об объекте, является \textit{энтропией}.

В теории информации энтропия -- \textit{степень неопределенности, связанная со случайной величиной.\footnote{\url{https://stackoverflow.com/questions/510412/what-is-the-computer-science-definition-of-entropy}}}

Также энтропию можно определить как \textit{наименьшее среднее число бит, необходимое для кодирования некоторой информации:}

\[H=-\sum\limits_{i=1}^n p_i\log p_i, \]
где $p_i$ --- вероятность $i$-го исхода. Или вероятность того, что в <<данетках>> угадываемый объект обладает некоторой характеристикой. Например, нужно угадать, какого человека загадали, и с вероятностью $2/3$ он моложе 30 лет, с вероятностью $1/3$ старше 30 лет. Такое может быть в ситуации, когда молодых людей среди тех, кого могли бы загадать, больше, или загадывающий отдает предпочтение более молодым людям. 

\subsection*{\hyperref[sec:sol_problem1]{Задача 1.}}\label{sec:problem1} 
Красная Шапочка встретила соседа-лесоруба по дороге к бабушке, которая равновероятно может жить в одной из трех деревень. Шапка точно помнит, в какой именно. Поскольку девочка маленькая, а неподалеку обитает волк, лесоруб решил узнать, в какой деревне живет бабушка, только не спросив напрямую, а задавая наводящие вопросы. 

Найдите энтропию местонахождения бабули. 

\section*{Еще немножко :)}~\
\\

\begin{siderules}
    \textbf{Условная энтропия} --- количество бит, необходимое для того, чтобы узнать значение случайной величины $Y$ при условии, что случайная величина $X$ известна.
\end{siderules}

Можно объяснить и проще --- вспомним вновь игру выше. Вам необходимо узнать, кого загадал человек, ведущий в <<данетке>>. Однако теперь он загадывает не одного человека, а \textit{пару}. Каждый человек в этой паре с равными вероятностями может быть как моложе 30 лет (в двух случаях из трех), так и старше 30 (в одном случае из трех). И нам известно, что точно загадали человека моложе 30 лет (одному человеку из этой пары меньше 30 лет). Это и будет наше условие $X$. А далее мы уже исходя из данной информации должны отгадать, кого же все-таки загадали?

Условная энтропия (среднее число вопросов, необходимое для того, чтобы понять, кого загадали) рассчитывается так:
\[H(Y|X)=-\sum\limits_{x, y} p(x, y)\log \cfrac{p(x, y)}{p(x)} \]

\subsection*{\hyperref[sec:sol_problem2]{Задача 2.}}\label{sec:problem2} Оказалось, на дороге в одну из трёх деревень, в каждой из которых равновероятно может находиться бабуля, ошивается злой волк Матвей, а в одну деревню ведет только одна дорога. Вероятности того, что Матвей находится в деревне $i$-той ($X$ --- местонахождение волка по вертикали), и того, что бабушка в деревне $j$-той ($Y$ --- местонахождение бабули по горизонтали):
\begin{center}
    \begin{tabular}{c||c|c|c}
        Деревня & 2112 & 2110 & К10 \\
        \hline
        \hline
        2112 & 1/4 & 1/24 & 1/24 \\
        \hline
        2110 & 1/24 & 1/4 & 1/24 \\
        \hline
        К10 & 1/24 & 1/24 & 1/4 \\
    \end{tabular}
\end{center}

Найдите условную энтропию местонахождения Матвея при неизвестном местонахождении бабули: $H(X|Y)$. 
\\

\begin{siderules}
    \textbf{Совместная энтропия} --- степень неопределенности, связанная со множеством случайных величин.
\end{siderules}

Как и ранее, ведущий загадал пару людей. Однако теперь мы ничего заранее не знаем, кроме вероятностей, с которыми могли загадать людей, обладающих определенными признаками. Иными словами, вероятность, с которой загадали человека моложе 30, вероятность, с которой волосы загаданного человека имеют рыжий оттенок, и так далее.

Совместная энтропия (среднее число вопросов, необходимое для отгадывания пары людей) рассчитывается так:
\[H(X, Y)=-\sum\limits_{x}\sum\limits_{y} p(x, y)\log p(x ,y) \]

\subsection*{\hyperref[sec:sol_problem3]{Задача 3.}}\label{sec:problem3} Оказалось, на дороге в одну из трёх деревень, в каждой из которых равновероятно может находиться бабуля, ошивается злой волк Матвей, а в одну деревню ведет только одна дорога. Вероятности того, что Матвей находится в деревне $i$-той ($X$ --- местонахождение волка по вертикали), и того, что бабушка в деревне $j$-той ($Y$ --- местонахождение бабули по горизонтали):
\begin{center}
    \begin{tabular}{c||c|c|c}
        Деревня & 2112 & 2110 & К10 \\
        \hline
        \hline
        2112 & 1/4 & 1/24 & 1/24 \\
        \hline
        2110 & 1/24 & 1/4 & 1/24 \\
        \hline
        К10 & 1/24 & 1/24 & 1/4 \\
    \end{tabular}
\end{center}

Найдите совместную энтропию местонахождения бабушки и Матвея: $H(X, Y)$. \\


\begin{siderules}
    \textbf{Взаимная информация} $I(X; Y)$ --- мера взаимной зависимости двух случайных величин.
\end{siderules}

Другими словами, \textbf{взаимная информация} --- это те сведения, которые мы получаем, отгадав одного из людей в паре, когда играем в <<данетки>>. Так, чтобы отгадать одного из людей в паре ($X$), в среднем необходимо $H(X)$ вопросов. Чтобы отгадать этого человека, однако зная, что второй загаданный это $Y$, в среднем необходимо $H(X|Y)$ вопросов. Тогда разница в количестве вопросов и есть \textit{взаимная информация}, или те вопросы, которые уже не нужно задавать, если известен один из людей в паре. Также совместная информация обладает свойством \textit{симметричности}: $I(X;Y)=I(Y;X)$. Интуитивно это объясняется тем, что если нам известен один человек из загаданной пары, то часть вопросов (число которых в среднем равно $H(X)$) задавать уже не имеет смысла, поскольку ответ на них следует из уже полученной информации об известном человеке. 

Рассчитывается она так:
\[I(X; Y)=H(X)-H(X|Y)=\]
\[=-\sum\limits_{x} p(x)\log p(x) +\sum\limits_{x, y} p(x, y)\log \cfrac{p(x, y)}{p(y)} =
\sum\limits_{x}\sum\limits_{y} p(x, y)\log \cfrac{p(x, y)}{p(x)p(y)} \]

\subsection*{\hyperref[sec:sol_problem4]{Задача 4.}}\label{sec:problem4}  Оказалось, на дороге в одну из трёх деревень, в каждой из которых равновероятно может находиться бабуля, ошивается злой волк Матвей, а в одну деревню ведет только одна дорога. Вероятности того, что Матвей находится в деревне $i$-той ($X$ --- местонахождение волка по вертикали), и того, что бабушка в деревне $j$-той ($Y$ --- местонахождение бабули по горизонтали):
\begin{center}
    \begin{tabular}{c||c|c|c}
        Деревня & 2112 & 2110 & К10 \\
        \hline
        \hline
        2112 & 1/4 & 1/24 & 1/24 \\
        \hline
        2110 & 1/24 & 1/4 & 1/24 \\
        \hline
        К10 & 1/24 & 1/24 & 1/4 \\
    \end{tabular}
\end{center}

Найдите взаимную информацию местонахождения Матвея и Бориса: $I(X; Y)$
\\

\textbf{Все упомянутые выше герои обладают следующими свойствами:}

\begin{itemize}
	\item $H \geqslant 0$
	\item $H(Y|X)=H(X, Y)-H(X)$ 
	\item $H(Y|X)\leqslant H(Y)$
	\item $H(X, Y)=H(X|Y)+H(Y|X)+I(X; Y)=H(X)+H(Y)-I(X; Y)$, где $I(X; Y)$ -- взаимная информация о случайных величинах $X$ и $Y$
	\item $I(X; Y)\leqslant H(X)$, \ $I(X; Y)=H(X)-H(X|Y)$
\end{itemize}

\section*{А есть еще кросс энтропия!}~\
\\

\begin{siderules}
    \textbf{Кросс энтропия} --- минимальное среднее количество бит, необходимое для того, чтобы закодировать некоторую информацию, если схема кодирования базируется на некотором распределении $q$, а не истинном, $p$.
\end{siderules}

\[CE(P||Q)=-\sum\limits_{i=1}^{n}p_i\log q_i \]

Также через кросс энтропию можно определить \textit{дивергенцию Кульбака -- Лейблера}. Для начала стоит узнать, что это:

\begin{siderules}
    \textbf{Дивергенция Кульбака -- Лейблера} --- степень отдаленности одного вероятностного распределения от другого (называется также \textit{относительная энтропия}). \end{siderules}
    
    Интуитивная интерпретация данного понятия такова: насколько больше вопросов мы зададим в случае неоптимальной стратегии игры в <<данетки>>, чем в случае оптимальной стратегии. Можно заметить, что дивергенция Кульбака -- Лейблера равна разности кросс-энтропии и энтропии, так как на кодирование информации при неоптимальной стратегии уйдет гораздно больше бит, чем при оптимальной. Мы ищем именно ту разницу, насколько неоптимальная стратегия хуже оптимальной: 
    
    \[D_{KL}(P\, ||\, Q)= CE(P||Q) - H(p), \text{ или } CE(P||Q)=H(p)+D_{KL}(P\, || \, Q)\]
    
    Рассчитывается для дискретного случая так:

    \[D_{KL}(P\, ||\, Q)=  - \sum\limits_{i=1}^n p_i\log q_i\ - ( - \sum\limits_{i=1}^n p_i\log p_i)\]



\subsection*{\hyperref[sec:sol_problem5]{Задача 5.}}\label{sec:problem5} Красная Шапочка, убегая от злого лесоруба, в панике перепутала вероятности, с которыми охотник Борис находится в одной из деревень ($X$ --- местонахождение охотника):
\[\begin{pmatrix}
    1/6 & 2/3 & 1/6
\end{pmatrix} , \] и с которыми волк Матвей ошивается на одной из дорог в деревни ($Y$ --- местонахождение волка):
\[\begin{pmatrix}
    3/8 & 3/8 & 1/4 
\end{pmatrix} .\]

(а) Найдите кросс-энтропию из истинного распределения местонахождения Матвея в распределение местонахождения Бориса;

(б) Вычислите дивергенцию Кульбака-Лейблера.

\section*{А что, только для дискретных случайных величин?}~\
\\

Нет! :)

В случае, если Вы работаете с абсолютно непрерывными случайными величинами, энтропия и её родственники расчитываются по следующим формулам:
\begin{itemize}
    \item Самая главная и простая энтропийка:
    \[H(X)=-\int\limits_{-\infty}^{+\infty} f(x)\log f(x)dx \]
    \item Условная энтропия:
    \[H(Y|X)=-\int\limits_{-\infty}^{+\infty} f(x, y)\log f_{Y|X}(y)dy \]
    \item Совместная энтропия:
    \[H(X, Y)=-\int\limits_{-\infty}^{+\infty} \int\limits_{-\infty}^{+\infty} f(x, y)\log f(x, y)dxdy \]
    \item Взаимная информация:
    \[I(X; Y)=\int\limits_{-\infty}^{+\infty} \int\limits_{-\infty}^{+\infty} f(x, y) \log \cfrac{f(x, y)}{f(x)f(y)}dxdy \]
    \item Кросс-энтропия:
    \[CH(p, q)=-\int\limits_{-\infty}^{+\infty}p(x)\log q(x) dx \]
    \item Дивергенция Кульбака -- Лейблера:
    \[D_{KL}(P\, ||\, Q)=\int\limits_{-\infty}^{+\infty} p(x)\log p(x)dx -\int\limits_{-\infty}^{+\infty} p(x)\log q(x)dx  \]
\end{itemize}

\subsection*{\hyperref[sec:sol_problem6]{Задача 6.}}\label{sec:problem6} На дороге в деревню, где живет бабушка, Красная Шапка обронила серебрянную монетку, местонахождение которой распределено равномерно. Дорога в эту деревню располагается на отрезке $[0; A]$. Шапка решила сообщить бабуле энтропию местонахождения потерянной монетки. Помогите бедной Шапочке её посчитать.

\subsection*{\hyperref[sec:sol_problem7]{Задача 7.}}\label{sec:problem7} Злой лесоруб решил, что он должен завладеть сердцем Красной Шапки и устранить со своего пути её бабушку, которая против их отношений. Лесоруб не знает, где именно находится бабушка.

Бабушка ест ягодки. Местоположение кустика с ягодками $X$ и местоположение ямы $Y$, которую выкопала Красная Шапочка для деревца, отлично описываются многомерным нормальным распределением:
\[\begin{pmatrix} 
    X \\ 
    Y 
  \end{pmatrix}\sim \mathcal{N}\left(\begin{pmatrix} 
      1 \\ 
      0
    \end{pmatrix},
    \begin{pmatrix} 
      4 & 1 \\ 
      1 & 1
    \end{pmatrix}\right)\]

Какова условная энтропия местоположения бабушки в зависимости от местоположения ямы?

\subsection*{\hyperref[sec:sol_problem8]{Задача 8.}}\label{sec:problem8} Злой лесоруб решил, что он должен завладеть сердцем Красной Шапки и устранить со своего пути её бабушку, которая против их отношений. Лесоруб не знает, где именно находится бабушка.

Бабушка ест ягодки. Местоположение кустика с ягодками $X$ и местоположение ямы $Y$, которую выкопала Красная Шапочка для деревца, отлично описываются многомерным нормальным распределением:
\[\begin{pmatrix} 
    X \\ 
    Y 
  \end{pmatrix}\sim \mathcal{N}\left(\begin{pmatrix} 
      1 \\ 
      0
    \end{pmatrix},
    \begin{pmatrix} 
      4 & 1 \\ 
      1 & 1
    \end{pmatrix}\right)\]

Какова совместная энтропия местоположения бабушки и местоположения ямы?

\subsection*{\hyperref[sec:sol_problem9]{Задача 9.}}\label{sec:problem9} Злой лесоруб решил, что он должен завладеть сердцем Красной Шапки и устранить со своего пути её бабушку, которая против их отношений. Лесоруб не знает, где именно находится бабушка.

Бабушка ест ягодки. Местоположение кустика с ягодками $X$ и местоположение ямы $Y$, которую выкопала Красная Шапочка для деревца, отлично описываются многомерным нормальным распределением:
\[\begin{pmatrix} 
    X \\ 
    Y 
  \end{pmatrix}\sim \mathcal{N}\left(\begin{pmatrix} 
      1 \\ 
      0
    \end{pmatrix},
    \begin{pmatrix} 
      4 & 1 \\ 
      1 & 1
    \end{pmatrix}\right)\]

Какова взаимная информация местоположения бабушки и местоположения ямы?

