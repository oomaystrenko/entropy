\section*{Что это такое?}~\
\\ 

Вспомним всем известную игру <<данетки>>. Так, чтобы понять, о чем идет речь, мы задаем уточняющие вопросы. Так вот, минимальное число вопросов, необходимое, чтобы выяснить полную информацию об объекте, является \textit{энтропией}.

В теории информации энтропия -- \textit{степень неопределенности, связанная со случайной величиной.\footnote{\url{https://stackoverflow.com/questions/510412/what-is-the-computer-science-definition-of-entropy}}}

Также энтропию можно определить как \textit{наименьшее среднее число бит, необходимое для кодирования некоторой информации:}

\[H=-\sum\limits_{i=1}^n p_i\log p_i \]
где $p_i$ --- вероятность $i$-го исхода. Или вероятность того, что в <<данетки>> угадываемый объект обладает некоторой характеристикой. Например, нужно угадать, какого человека загадали, и с вероятностью $2/3$ он моложе 30 лет, с вероятностью $1/3$ старше 30 лет. Такое может быть в ситуации, когда молодых людей среди тех, кого могли бы загадать, больше, или загадывающий отдает предпочтение более молодым людям. 

\subsection*{\hyperref[sec:sol_problem1]{Задача 1.}}\label{sec:problem1} 
Красная Шапочка должна отнести бабушке пирожки. Шапочка не помнит, где живет бабушка, но помнит, что равновероятно в одной из трех деревень. Также известно, что в одной из деревень равновероятно находится волк. Если Шапка и волк встретятся, то бабушка не получит пирожки. 
1. Посчитайте количество возможных вариантов, в которых бабушка получит пирожки при условии, что волк не поймает Шапку; 
2. Посчитайте энтропию события из прошлого пункта. 

\section*{Еще немножко :)}~\
\\

\begin{siderules}
    \textbf{Условная энтропия} --- количество бит, необходимое для того, чтобы узнать имеющуюся информацию о случайной величине $Y$ при условии, что случайная величина $X$ известна.
\end{siderules}

Можно объяснить и проще --- вспомним вновь игру выше. Вам необходимо узнать, кого загадал человек, ведущий в <<данетке>>. Однако теперь он загадывает не одного человека, а \textit{пару}. Каждый человек в этой паре с равными вероятностями может быть как моложе 30 лет (в двух случаях из трех), так и старше 30 (в одном случае из трех). И нам известно, что точно загадали человека моложе 30 лет (одному человеку из этой пары меньше 30 лет). Это и будет наше условие $X$. А далее мы уже исходя из данной информации должны отгадать, кого же все-таки загадали?

Энтропия (среднее число вопросов, необходимое для того, чтобы понять, кого загадали) рассчитывается так:
\[H(Y|X)=-\sum\limits_{x\in X, y\in Y} p(x, y)\log \cfrac{p(x, y)}{p(x)} \]

\subsection*{\hyperref[sec:sol_problem2]{Задача 2.}}\label{sec:problem2} Красная Шапочка должна отнести бабушке пирожки. Шапочка не помнит, где живет бабушка, но помнит, что что равновероятно в одной из трех деревень. Также известно, что около деревень ошивается целых $N$ волков ($N\leqslant3$), при этом возле каждой деревни либо один волк, либо нет волков. Волков ловит храбрый охотник, он равновероятно находится в одной из трех деревень. Волк поймает Шапку, если рядом не будет охотника.
1. Посчитайте вероятность того, что бабушка получит пирожки при условии того, что волк не поймал Шапку; 
2. Посчитайте условную энтропию местонахождения бабушки в зависимости от $N$.  
\\

\begin{siderules}
    \textbf{Совместная энтропия} --- степень неопределенности, связанная со множеством случайных величин.
\end{siderules}

Как и ранее, ведущий загадал пару людей. Однако теперь мы ничего заранее не знаем, кроме вероятностей, с которыми могли загадать людей, обладающих определенными признаками. Иными словами, вероятность, с которой загадали человека моложе 30, вероятность, с которой волосы загаданного человека имеют рыжий оттенок, и так далее.

Совместная энтропия (среднее число вопросов, необходимое для отгадывания пары людей) рассчитывается так:
\[H(X, Y)=-\sum\limits_{x\in X}\sum\limits_{y\in Y} p(x, y)\log p(x ,y) \]

\subsection*{\hyperref[sec:sol_problem3]{Задача 3.}}\label{sec:problem3} Красная Шапочка должна отнести бабушке пирожки. Красная Шапочка не помнит, где живет бабушка, но помнит, что в деревне А с вероятностью 1/2, а в Б и В – с 1/4. Также известно, что в одной из деревень равновероятно находится волк. Посчитайте совместную энтропию местонахождения бабушки и волка в такой ситуации.
\\ 

Все упомянутые выше герои обладают следующими свойствами:

\begin{itemize}
    \item $H \geqslant 0$
    \item $H(Y|X)=H(X, Y)-H(X)$ или в более общем случае $H(X_1, \ldots, X_n)=\sum\limits_{i=1}^n H(X_i|X_1, \ldots, X_n)$
    \item $H(Y|X)\leqslant H(Y)$
    \item $H(X, Y)=H(X|Y)+H(Y|X)+I(X; Y)=H(X)+H(Y)-I(X; Y)$, где $I(X; Y)$ -- взаимная информация о случайных величинах $X$ и $Y$
    \item $I(X; Y)\leqslant H(X)$
\end{itemize}

\begin{siderules}
    \textbf{Взаимная информация} --- мера взаимной зависимости двух случайных величин.
\end{siderules}

Другими словами, \textbf{взаимная информация} --- это то, что нам известно о загаданной паре. Если ведущий сначала выбрал одного человека в паре, а затем подобрал второго так, чтобы они как-то были похожи друг на друга или, наоборот, максимально отличались, то информацию о паре можно вычислить, узнав всю информацию о первом человеке в этой паре, затем о втором, сложив их и вычтя те сведения, которые осведомляют о признаках сразу обоих людей.

Рассчитывается она так:
\[I(X; Y)=\sum\limits_{x\in X}\sum\limits_{y\in Y} p(x, y)\log \cfrac{p(x, y)}{p(x)p(y)} \]

\subsection*{\hyperref[sec:sol_problem4]{Задача 4.}}\label{sec:problem4}  Красная Шапочка должна отнести бабушке пирожки. Шапочка не помнит, где живет бабушка, но помнит, что что равновероятно в одной из трех деревень. Однако, когда Шапка уже проделала половину пути, мама услышала по новостям о волке, который с одинаковой вероятностью может находиться в одной из трех деревень! Обеспокоенная мама нацепила на голову платок и побежала спасать свою дочурку, она точно знала, что охотник сегодня поджидает волка в деревне В, а значит туда ей нет смысла идти. После долгого дня и мама, и Красная шапочка сидели дома у бабушки и все вместе ели пирожки. Посчитайте взаимную информацию этих двух событий (мама и дочка благополучно добрались по пункта назначения).

\section*{А есть еще кросс энтропия!}~\
\\

\begin{siderules}
    \textbf{Кросс энтропия} --- минимальное среднее количество бит, необходимое для того, чтобы закодировать некоторую информацию, если схема кодирования базируется на некотором распределении $q$, а не истинном, $p$.
\end{siderules}

\[CH(p, q)=-\sum\limits_{i=1}^{n}p_i\log q_i \]

\subsection*{\hyperref[sec:sol_problem5]{Задача 5.}}\label{sec:problem5} Красная Шапочка должна отнести бабушке пирожки. Шапочка думает, что в корзинке 20 пирожков с капустой и 20 --- с вареньем, но её мама все перепутала и вместо этого положила 10 с капустой и 30 с вареньем. По дороге к бабушке Шапочка решила съесть один пирожок. Он оказался с капустой. Посчитайте кросс энтропию такого события.
\\

Также кросс энтропию можно определить через \textit{расстояние Кульбака -- Лейблера}. Для начала стоит узнать, что это:

\begin{siderules}
    \textbf{Расстояние Кульбака -- Лейблера} --- степень отдаленности друг от друга двух вероятностных распределений (называется также \textit{относительная энтропия}). \end{siderules}
    
    Рассчитывается для дискретного случая так:

    \[D(P\, ||\, Q)=\sum\limits_{i=1}^n p_i\log p_i-\sum\limits_{i=1}^n p_i\log q_i\]

Нетрудно заметить, что расстояние Кульбака -- Лейблера равно разности энтропии и кросс-энтропии:

\[D(P\, ||\, Q)=H(p)-CH(p,q), \text{ или } CH(p, q)=H(p)+D_{KL}(p\, || \, q)\]

\subsection*{\hyperref[sec:sol_problem6]{Задача 6.}}\label{sec:problem6}

\section*{А что, только для дискретных случайных величин?}~\
\\

Нет! :)

В случае, если Вы работаете с абсолютно непрерывными случайными величинами, энтропия и её родственники расчитываются по следующим формулам:
\begin{itemize}
    \item Самая главная и простая энтропийка:
    \[H(X)=-\int\limits_{-\infty}^{+\infty} f(x)\log f(x)dx \]
    \item Условная энтропия:
    \[H(Y|X)=-\int\limits_{-\infty}^{+\infty} f(x, y)\log f_{Y|X}(y)dy \]
    \item Совместная энтропия:
    \[H(X, Y)=-\int\limits_{-\infty}^{+\infty} \int\limits_{-\infty}^{+\infty} f(x, y)\log f(x, y)dxdy \]
    \item Взаимная информация:
    \[I(X; Y)=\int\limits_{-\infty}^{+\infty} \int\limits_{-\infty}^{+\infty} f(x, y) \log \cfrac{f(x, y)}{f(x)f(y)}dxdy \]
    \item Кросс-энтропия:
    \[CH(p, q)=-\int\limits_{-\infty}^{+\infty}p(x)\log q(x) dx \]
    \item Расстояние Кульбака -- Лейблера:
    \[D(P\, ||\, Q)=\int\limits_{-\infty}^{+\infty} f(x)\log f(x)dx -\int\limits_{-\infty}^{+\infty} f(x)\log g(x)dx  \]
\end{itemize}

В следующих шести задачах известно, что деревни равномерно распределены на отрезке от дома шапочки (точка отсчета 0) до точки $N$ (никто не знает, что находится за ее пределами...)

\subsection*{\hyperref[sec:sol_problem7]{Задача 7.}}\label{sec:problem7}
Красная Шапочка должна отнести бабушке пирожки. Шапочка не помнит, где живет бабушка, но помнит, что что равновероятно в одной из трех деревень.  Посчитайте энтропию того, что внучка все-таки встретилась с бабушкой. 

\subsection*{\hyperref[sec:sol_problem8]{Задача 8.}}\label{sec:problem8} 
Красная Шапочка должна отнести бабушке пирожки. Шапочка не помнит, где живет бабушка, но помнит, что что равновероятно в одной из трех деревень. Однако, жители деревень стали поговаривать о волке, который завелся в лесу. Местонахождение волка определяется экспоненциальным распределением с параметром $\lambda$. Одновременно с появлением волка на помощь беззащитным жителям деревень пришел охотник, местоположение которого равномерно распределено на отрезке $(0, N)$. Найдите энтропию того, бабушка получила свои пирожки при условии того, что волк был пойман.

\subsection*{\hyperref[sec:sol_problem9]{Задача 9.}}\label{sec:problem9} 
Основываясь на данных из предыдущей задачи посчитайте совместную энтропию того, что Шапочка дошла до бабушки и волк был пойман.

\subsection*{\hyperref[sec:sol_problem10]{Задача 10.}}\label{sec:problem10} 
Красная Шапочка должна отнести бабушке пирожки. Шапочка не помнит, где живет бабушка, но помнит, что что равновероятно в одной из трех деревень. Однако, жители деревень стали поговаривать о волке, который завелся в лесу. Местонахождение волка определяется экспоненциальным распределением с параметром лямбда.  Охотник использует новомодную тактику боя, поэтому функция плотности местоположения охотника задается экспоненциальным распределением с параметром $2\lambda$. Известно, что в походе шапочка не дошла до бабушки (о ужас!), но зато волк был пойман. Рассчитайте взаимную информацию двух этих событий (шапка не нашла бабушку и охотник поймал волка). 

\subsection*{\hyperref[sec:sol_problem11]{Задача 11.}}\label{sec:problem11}
Красная Шапочка должна отнести бабушке пирожки. Пока Красная Шапочка бежала к бабушке из её корзинки в какой-то момент начали выпадать пирожки. Всего в корзинке их было $N$ штук. Известно, что пирожки падали равномерно на некоторый участок дороги. Потерю всех пирожков Шапка обнаружила лишь по прибытии к бабушке и сразу решила собрать все выпавшие пирожки. Предполагая, что расстояние от дома Шапочки до дома Бабушки равно $a$, рассчитайте кросс-энтропию, если:

(а) Шапочка знает, что пирожки выпадали равномерно;

(б) Шапочка не знает, что пирожки выпадали равномерно;

(в) Какая из величин больше?

\subsection*{\hyperref[sec:sol_problem12]{Задача 12.}}\label{sec:problem12} 

