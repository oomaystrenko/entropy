\section*{Что это такое?}~\
\\ 

Вспомним всем известную игру <<данетки>>. Так, чтобы понять, о чем идет речь, мы задаем уточняющие вопросы. Так вот, минимальное число вопросов, необходимое, чтобы выяснить полную информацию об объекте, является \textit{энтропией}.

В теории информации энтропия -- \textit{степень неопределенности, связанная со случайной величиной.\footnote{\url{https://stackoverflow.com/questions/510412/what-is-the-computer-science-definition-of-entropy}}}

Также энтропию можно определить как \textit{наименьшее среднее число бит, необходимое для кодирования некоторой информации:}

\[H=-\sum\limits_{i=1}^n p_i\log p_i \]
где $p_i$ --- вероятность $i$-го исхода. Или вероятность того, что в <<данетки>> угадываемый объект обладает некоторой характеристикой. Например, нужно угадать, какого человека загадали, и с вероятностью $2/3$ он моложе 30 лет, с вероятностью $1/3$ старше 30 лет. Такое может быть в ситуации, когда молодых людей среди тех, кого могли бы загадать, больше, или загадывающий отдает предпочтение более молодым людям. 

\subsection*{\hyperref[sec:sol_problem1]{Задача 1.}}\label{sec:problem1} 
Красная Шапочка должна отнести бабушке пирожки. Шапочка не помнит, где живет бабушка, но помнит, что равновероятно в одной из трех деревень. Также известно, что в одной из деревень равновероятно находится волк. Если Шапка и волк встретятся, то бабушка не получит пирожки. 

1. Посчитайте количество возможных вариантов, в которых бабушка получит пирожки при условии, что волк не поймает Шапку; 

2. Посчитайте энтропию события из прошлого пункта. 

\section*{Еще немножко :)}~\
\\

\begin{siderules}
    \textbf{Условная энтропия} --- количество бит, необходимое для того, чтобы узнать имеющуюся информацию о случайной величине $Y$ при условии, что случайная величина $X$ известна.
\end{siderules}

Можно объяснить и проще --- вспомним вновь игру выше. Вам необходимо узнать, кого загадал человек, ведущий в <<данетке>>. Однако теперь он загадывает не одного человека, а \textit{пару}. Каждый человек в этой паре с равными вероятностями может быть как моложе 30 лет (в двух случаях из трех), так и старше 30 (в одном случае из трех). И нам известно, что точно загадали человека моложе 30 лет (одному человеку из этой пары меньше 30 лет). Это и будет наше условие $X$. А далее мы уже исходя из данной информации должны отгадать, кого же все-таки загадали?

Энтропия (среднее число вопросов, необходимое для того, чтобы понять, кого загадали) рассчитывается так:
\[H(Y|X)=-\sum\limits_{x\in X, y\in Y} p(x, y)\log \cfrac{p(x, y)}{p(x)} \]

\subsection*{\hyperref[sec:sol_problem2]{Задача 2.}}\label{sec:problem2} Красная Шапочка должна отнести бабушке пирожки. Шапочка не помнит, где живет бабушка, но помнит, что что равновероятно в одной из трех деревень. Также известно, что около деревень ошивается целых $N$ волков ($N\leqslant3$), при этом возле каждой деревни либо один волк, либо нет волков. Волков ловит храбрый охотник, он равновероятно находится в одной из трех деревень. Волк поймает Шапку, если рядом не будет охотника.

1. Посчитайте вероятность того, что бабушка получит пирожки при условии того, что волк не поймал Шапку; 

2. Посчитайте условную энтропию местонахождения бабушки в зависимости от $N$.  
\\

\begin{siderules}
    \textbf{Совместная энтропия} --- степень неопределенности, связанная со множеством случайных величин.
\end{siderules}

Как и ранее, ведущий загадал пару людей. Однако теперь мы ничего заранее не знаем, кроме вероятностей, с которыми могли загадать людей, обладающих определенными признаками. Иными словами, вероятность, с которой загадали человека моложе 30, вероятность, с которой волосы загаданного человека имеют рыжий оттенок, и так далее.

Совместная энтропия (среднее число вопросов, необходимое для отгадывания пары людей) рассчитывается так:
\[H(X, Y)=-\sum\limits_{x\in X}\sum\limits_{y\in Y} p(x, y)\log p(x ,y) \]

\subsection*{\hyperref[sec:sol_problem3]{Задача 3.}}\label{sec:problem3} Красная Шапочка должна отнести бабушке пирожки. Красная Шапочка не помнит, где живет бабушка, но помнит, что в деревне А с вероятностью 1/2, а в Б и В – с 1/4. Также известно, что в одной из деревень равновероятно находится волк. Посчитайте совместную энтропию местонахождения бабушки и волка в такой ситуации.
\\ 

Все упомянутые выше герои обладают следующими свойствами:

\begin{itemize}
    \item $H \geqslant 0$
    \item $H(Y|X)=H(X, Y)-H(X)$ или в более общем случае $H(X_1, \ldots, X_n)=\sum\limits_{i=1}^n H(X_i|X_1, \ldots, X_n)$
    \item $H(Y|X)\leqslant H(Y)$
    \item $H(X, Y)=H(X|Y)+H(Y|X)+I(X; Y)=H(X)+H(Y)-I(X; Y)$, где $I(X; Y)$ -- взаимная информация о случайных величинах $X$ и $Y$
    \item $I(X; Y)\leqslant H(X)$
\end{itemize}

\begin{siderules}
    \textbf{Взаимная информация} --- мера взаимной зависимости двух случайных величин.
\end{siderules}

Другими словами, \textbf{взаимная информация} --- это то, что нам известно о загаданной паре. Если ведущий сначала выбрал одного человека в паре, а затем подобрал второго так, чтобы они как-то были похожи друг на друга или, наоборот, максимально отличались, то информацию о паре можно вычислить, узнав всю информацию о первом человеке в этой паре, затем о втором, сложив их и вычтя те сведения, которые осведомляют о признаках сразу обоих людей.

Рассчитывается она так:
\[I(X; Y)=\sum\limits_{x\in X}\sum\limits_{y\in Y} p(x, y)\log \cfrac{p(x, y)}{p(x)p(y)} \]

\subsection*{\hyperref[sec:sol_problem4]{Задача 4.}}\label{sec:problem4}  Красная шапочка не помнит, где живет бабушка, но помнит, что либо в А, либо в Б. Также известно, что в одной из деревень А, Б или В равновероятно находится волк. Если Шапка и волк встретятся, то бабушка не получит пирожки. 
1.	Посчитайте вероятность того, что бабушка получит пирожки при условии, что волк не поймает Шапку;
2.	Посчитайте взаимную информацию двух событий: волк не поймал Шапку, и бабушка получила пирожки.

\section*{А есть еще кросс энтропия!}~\
\\

\begin{siderules}
    \textbf{Кросс энтропия} --- минимальное среднее количество бит, необходимое для того, чтобы закодировать некоторую информацию, если схема кодирования базируется на некотором распределении $q$, а не истинном, $p$.
\end{siderules}

\[CH(p, q)=-\sum\limits_{i=1}^{n}p_i\log q_i \]

Также кросс энтропию можно определить через \textit{расстояние Кульбака -- Лейблера}. Для начала стоит узнать, что это:

\begin{siderules}
    \textbf{Расстояние Кульбака -- Лейблера} --- степень отдаленности друг от друга двух вероятностных распределений (называется также \textit{относительная энтропия}). \end{siderules}
    
    Рассчитывается для дискретного случая так:

    \[D(P\, ||\, Q)=\sum\limits_{i=1}^n p_i\log p_i-\sum\limits_{i=1}^n p_i\log q_i\]

Нетрудно заметить, что расстояние Кульбака -- Лейблера равно разности энтропии и кросс-энтропии:

\[D(P\, ||\, Q)=H(p)-CH(p,q), \text{ или } CH(p, q)=H(p)+D_{KL}(p\, || \, q)\]

\subsection*{\hyperref[sec:sol_problem5]{Задача 5.}}\label{sec:problem5} Шапочка думала, что в корзинке 20 пирожков с капустой и 20 — с вареньем, но её мама все перепутала и вместо этого положила 10 с капустой и 30 с вареньем. По дороге к бабушке Шапочка решила съесть один пирожок. Он оказался с капустой. 
1.	Посчитайте кросс энтропию в такой ситуации;
2.	Найдите расстояние Кульбака-Лейблера.

\section*{А что, только для дискретных случайных величин?}~\
\\

Нет! :)

В случае, если Вы работаете с абсолютно непрерывными случайными величинами, энтропия и её родственники расчитываются по следующим формулам:
\begin{itemize}
    \item Самая главная и простая энтропийка:
    \[H(X)=-\int\limits_{-\infty}^{+\infty} f(x)\log f(x)dx \]
    \item Условная энтропия:
    \[H(Y|X)=-\int\limits_{-\infty}^{+\infty} f(x, y)\log f_{Y|X}(y)dy \]
    \item Совместная энтропия:
    \[H(X, Y)=-\int\limits_{-\infty}^{+\infty} \int\limits_{-\infty}^{+\infty} f(x, y)\log f(x, y)dxdy \]
    \item Взаимная информация:
    \[I(X; Y)=\int\limits_{-\infty}^{+\infty} \int\limits_{-\infty}^{+\infty} f(x, y) \log \cfrac{f(x, y)}{f(x)f(y)}dxdy \]
    \item Кросс-энтропия:
    \[CH(p, q)=-\int\limits_{-\infty}^{+\infty}p(x)\log q(x) dx \]
    \item Расстояние Кульбака -- Лейблера:
    \[D(P\, ||\, Q)=\int\limits_{-\infty}^{+\infty} f(x)\log f(x)dx -\int\limits_{-\infty}^{+\infty} f(x)\log g(x)dx  \]
\end{itemize}

\subsection*{\hyperref[sec:sol_problem6]{Задача 6.}}\label{sec:problem6} Красная шапочка идет от дома до бабушки и по дороге садится на пеньки, которые равномерно распределены на отрезке [0, В]. За Шапкой охотится волк, местонахождение которого определяется равномерным распределением на отрезке [0, В]. Известно, что волк съел девочку. Найдите энтропию расстояния, которое Шапке удалось пройти. 

\subsection*{\hyperref[sec:sol_problem7]{Задача 7.}}\label{sec:problem7} Красная шапочка идет от дома до бабушки и по дороге садится на пеньки, которые равномерно распределены на отрезке [0, В]. За ней охотится волк, местонахождение которого определяется равномерным распределением на том же отрезке. Территория от 0 до Х охраняется, поэтому если волк и Шапка встретятся до Х, девочку спасут. Известно, что волк съел Красную шапочку. Найдите условную энтропию расстояния, которое прошла девочка, при условии Х.

\subsection*{\hyperref[sec:sol_problem8]{Задача 8.}}\label{sec:problem8} Красная шапочка идет от дома до бабушки и по дороге садится на пеньки, которые равномерно распределены на отрезке [0, 10]. За ней охотится волк, местонахождение которого определяется следующей функцией плотности 
\begin{equation*}
f(X) = 
 \begin{cases}
   25x &\text{елси $x \in [0, 5]$}\\
   10 - 0.016x &\text{если $x \in (5, 10]$}
 \end{cases}
\end{equation*}
Территория от 0 до Х (Х<10) охраняется, поэтому если волк там окажется, его сразу же поймают. Посчитайте совместную энтропию следующих событий: Шапочка не встретила волка и волка поймали. 

\subsection*{\hyperref[sec:sol_problem9]{Задача 9.}}\label{sec:problem9} Красная шапочка идет от дома до бабушки и по дороге садится на пеньки, которые равномерно распределены на отрезке [0, 10]. За ней охотится волк, местонахождение которого определяется следующей функцией плотности
\begin{equation*}
f(X) = 
 \begin{cases}
   25x &\text{елси $x \in [0, 5]$}\\
   10 - 0.016x &\text{если $x \in (5, 10]$}
 \end{cases}
\end{equation*}
Территория от 0 до Х (Х<10) охраняется, поэтому если волк там окажется, его сразу же поймают. Шапка присела на пенек, который находится на расстоянии Y (Y < X). Посчитайте взаимную информацию следующих двух событий: шапочка не встретила волка и волка не поймали. 
 
\subsection*{\hyperref[sec:sol_problem10]{Задача 10.}}\label{sec:problem10} Красная Шапочка должна отнести бабушке пирожки. Пока Красная Шапочка бежала к бабушке из её корзинки в какой-то момент начали выпадать пирожки. Всего в корзинке их было $N$ штук. Известно, что пирожки падали равномерно на некоторый участок дороги. Потерю всех пирожков Шапка обнаружила лишь по прибытии к бабушке и сразу решила собрать все выпавшие пирожки. Предполагая, что расстояние от дома Шапочки до дома Бабушки равно $a$, рассчитайте кросс-энтропию, если:
(а) Шапочка знает, что пирожки выпадали равномерно;
(б) Шапочка не знает, что пирожки выпадали равномерно;
(в) Какая из величин больше?

\subsection*{\hyperref[sec:sol_problem11]{Задача 11.}}\label{sec:problem11}
