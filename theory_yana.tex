\section*{Что это такое?}~\
\\

Вспомним всем известную игру "данетки". Так, чтобы понять, о чем идет речь, мы задаем уточняющие вопросы. Так вот, минимальное число вопросов, необходимое, чтобы выяснить полную инофрмацию об объекте, является \textit{энтропией}.

В теории информации энтропия -- \textit{степень неопределенности, связанная со случайной величиной.\footnote{\url{https://stackoverflow.com/questions/510412/what-is-the-computer-science-definition-of-entropy}}}

Также энтропию можно определить как \textit{наименьшее среднее число бит, необходимое для кодирования некоторой информации.}

\[H=-\sum\limits_{i=1}^n p_i\log p_i \]
где $p_i$ -- вероятность $i$-го исхода. Или вероятность того, что в "данетках" угадываемый объект обладает некоторой характеристикой. Например, нужно угадать, какого человека загадали, и с вероятностью $2/3$ он моложе 30 лет, с вероятностью $1/3$ старше 30 лет. Такое может быть в ситуации, когда молодых людей среди тех, кого могли бы загадать, больше, или загадывающий отдает предпочтение более молодым людям. 

\subsection*{\hyperref[sec:sol_problem1]{Задача 1.}}\label{sec:problem1} 
Красная Шапочка должна отнести бабушке пирожки. В какой точно деревне сейчас живет бабушка, Шапка не знает, но выбирает она из трех: А, Б и В. Известно, что внучка отнесла пирожки туда, куда нужно. Посчитайте энтропию, если попав в какую-то деревню, Шапочка никогда не сможет выбраться из неё.

\subsection*{\hyperref[sec:sol_problem2]{Задача 2.}}\label{sec:problem2}  Через несколько недель мама снова попросила Шапочку отнести бабушке пирожки. Правда, за все это время произошло много нового. Во-первых, бабушка перекочевала в другую деревню (какую – неизвестно). Во-вторых, в лесу завелся волк, известно, что он находится где-то в окрестности деревни, но неизвестно, какой именно. Если Шапке по дороге встретится волк, то пирожки бабушка не получит… Известно, что все обошлось и Шапка смогла отыскать бабушку. Посчитайте энтропию, если вновь Шапочка, попав в одну из деревень, остается в ней навсегда.

\subsection*{\hyperref[sec:sol_problem3]{Задача 3.}}\label{sec:problem3} Перепуганная мама Красной Шапочки решила не рисковать здоровьем дочери и вызвала охотников, чтобы те поймали волка в одной из деревень. Когда все более-менее успокоилось, мама снова отправила дочку к бабушке. Однако, охотники еще не поймали волка, так как не могли его найти. А бабушка снова переехала в другую деревню. Если Шапочка окажется в одной деревне с волком, а охотников рядом не будет, то девочка провалит свою миссию. А если в это время охотники будут в той же деревне, что и волк, то они сразу же прибегут на помощь. Известно, что Шапочка смогла добраться до бабушки. Посчитайте энтропию, и вновь дорога в деревню -- дорога в один конец.

\section*{Еще немножко :)}~\
\\

\begin{siderules}
    \textbf{Условная энтропия} --- количество бит, необходимое для того, чтобы закодировать имеющуюся информацию о случайной величине $Y$ при условии, что случайная величина $X$ принимает определенное значение (или просто известна).
\end{siderules}

Можно объяснить и проще -- вспомним вновь игру выше. Вам необходимо узнать, кого загадал человек, ведущий в "данетке". Однако теперь он загадывает не одного человека, а \textit{пару}. Каждый человек в этой паре с равными вероятностями может быть как моложе 30 лет (в 2 случаях из 3), так и старше 30 (в 1 случае из 3). И нам известно, что точно загадали человека моложе 30 лет (одному человеку из этой пары меньше 30 лет). Это и будет наше условие $X$. А далее мы уже исходя из данной информации должны отгадать, кого же все-таки загадали?

Рассчитывается так:
\[H(Y|X)=-\sum\limits_{x\in X, y\in Y} p(x, y)\log \cfrac{p(x, y)}{p(x)} \]

\subsection*{\hyperref[sec:sol_problem4]{Задача 4.}}\label{sec:problem4} Шапка вновь отправилась на встречу к бабушке с корзинкой пирожков. И, конечно же, внучка не знала, в какой деревне на этот раз осела бабуля. Также Шапка не знала, что около деревень ошивалось целых $N$ волков ($N\leqslant3$), при этом возле каждой деревни либо был один волк, либо не было волков вообще. На борьбу с хищниками вышел один храбрый охотник, но находится он мог только в одной из трех деревень. Если Шапке не посчастливится, и она встретит волка, а помощь не подоспеет, то бабушка не получит свои пирожки. Известно, что Красной Шапочке снова удалось добраться до бабушки. Попади она в другую деревню, бабушка никогда бы не поела пирожки и, возможно, Шапку съел бы волк. Посчитайте условную энтропию в зависимости от $N$. 
\\

\subsection*{\hyperref[sec:sol_problem5]{Задача 5.}}\label{sec:problem5} За обедом бабушка решила съесть два пирожка. Известно, что всего Шапочка принесла ей 20 пирожков с капустой и 30 пирожков с вареньем. Известно, что второй пирожок был с капустой. Посчитайте энтропию при условии, что первый пирожок тоже был с капустой.

\begin{siderules}
    \textbf{Совместная энтропия} --- степень неопределенности, связанная со множеством случайных величин.
\end{siderules}

Как и ранее, ведущий загадал пару людей. Однако теперь мы ничего заранее не знаем, кроме вероятностей, с которыми могли загадать людей, обладающих определенными признаками. Иными словами, вероятность, с которой загадали человека моложе 30, вероятность, с которой волосы загаданного человека имеют рыжий оттенок, и так далее.

Формула для рассчета:
\[H(X, Y)=-\sum\limits_{x\in X}\sum\limits_{y\in Y} p(x, y)\log p(x ,y) \]

\subsection*{Задача 6.} Охотники изловили почти всех волков, кроме одного. Красная шапочка снова направилась к бабушке с гостинцами. Из ее похода стало известно, что она смогла дойти до бабушки, но при этом охотники так и не поймали волка. Посчитайте совместную энтропию. 

\subsection*{Задача 7.} Девочка по возвращению домой поняла, что оставила у бабушки свою красивую красную шапку, поэтому на следующий день направилась обратно. За это время все персонажи успели поменять свое место нахождения. Получилось так, что и бабушка, и волк, и охотник оказались в одной деревне, поэтому волк не поймал Шапку, а внучка и бабушка благополучно встретились. Посчитайте совместную энтропию этих событий. \\

Все упомянутые выше герои обладают следующими свойствами:

\begin{itemize}
    \item $H \geqslant 0$
    \item $H(Y|X)=H(X, Y)-H(X)$ или в более общем случае $H(X_1, \ldots, X_n)=\sum\limits_{i=1}^n H(X_i|X_1, \ldots, X_n)$
    \item $H(Y|X)\leqslant H(Y)$
    \item $H(X, Y)=H(X|Y)+H(Y|X)+I(X; Y)=H(X)+H(Y)-I(X; Y)$, где $I(X; Y)$ -- взаимная информация о случайных величинах $X$ и $Y$
    \item $I(X; Y)\leqslant H(X)$
\end{itemize}

\begin{siderules}
    \textbf{Взаимная информация} --- мера взаимной зависимости двух случайных величин.
\end{siderules}

Другими словами, \textbf{взаимная информация} --- это то, что нам известно о загаданной паре. Если ведущий сначала выбрал одного человека в паре, а затем подобрал второго так, чтобы они как-то были похожи друг на друга или, наоборот, максимально отличались, то информацию о паре можно вычислить, узнав всю информацию о первом человеке в этой паре, затем о втором, сложив их и вычтя те сведения, которые осведомляют о признаках сразу обоих людей.

Рассчитывается она так:
\[I(X; Y)=\sum\limits_{x\in X}\sum\limits_{y\in Y} p(x, y)\log \cfrac{p(x, y)}{p(x)p(y)} \]

\section*{А есть еще кросс энтропия!}~\
\\

\begin{siderules}
    \textbf{Кросс энтропия} --- минимальное среднее количество бит, необходимое для того, чтобы закодировать некоторую информацию, если схема кодирования базируется на некотором распределении $q$, а не истинном, $p$.
\end{siderules}

\[CH(p, q)=-\sum\limits_{i=1}^{n}p_i\log q_i \]

\subsection*{Задача 8.} Шапочка думала, что в корзинке 20 пирожков с капустой и 20 – с вареньем, но ее мама все перепутала и вместо этого положила 10 с капустой и 30 с вареньем. По дороге к бабушке Шапочка решила съесть один пирожок. Он оказался с капустой. Посчитайте кросс энтропию.\\

Также кросс энтропию можно определить через \textit{расстояние Кульбака -- Лейблера}. Для начала стоит узнать, что это:

\begin{siderules}
    \textbf{Расстояние Кульбака -- Лейблера} --- степень отдаленности друг от друга двух вероятностных распределений (называется также \textit{относительная энтропия}). \end{siderules}
    
    Рассчитывается для дискретного случая так:

    \[D(P\, ||\, Q)=\sum\limits_{i=1}^n p_i\log p_i-\sum\limits_{i=1}^n p_i\log q_i\]

Нетрудно заметить, что расстояние Кульбака -- Лейблера равно разности энтропии и кросс-энтропии:

\[D(P\, ||\, Q)=H(p)-CH(p,q),\]
или
\[CH(p, q)=H(p)+D_{KL}(p\, || \, q)\]

\section*{А что, только для дискретных случайных величин?}~\
\\

Нет! :)

В случае, если Вы работаете с абсолютно непрерывными случайными величинами, энтропия и её родственники расчитываются по следующим формулам:

\begin{itemize}
    \item Самая главная и простая энтропийка:
    \[H(X)=-\int\limits_{-\infty}^{+\infty} f(x)\log f(x)dx \]
    \item Условная энтропия:
    \[H(Y|X)=-\int\limits_{-\infty}^{+\infty} f(x, y)\log f_{Y|X}(y)dy \]
    \item Совместная энтропия:
    \[H(X, Y)=-\int\limits_{-\infty}^{+\infty} \int\limits_{-\infty}^{+\infty} f(x, y)\log f(x, y)dxdy \]
    \item Взаимная информация:
    \[I(X; Y)=\int\limits_{-\infty}^{+\infty} \int\limits_{-\infty}^{+\infty} f(x, y) \log \cfrac{f(x, y)}{f(x)f(y)}dxdy \]
    \item Кросс-энтропия:
    \[CH(p, q)=-\int\limits_{-\infty}^{+\infty}p(x)\log q(x) dx \]
    \item Расстояние Кульбака-Лейблера:
    \[D(P\, ||\, Q)=\int\limits_{-\infty}^{+\infty} f(x)\log f(x)dx -\int\limits_{-\infty}^{+\infty} f(x)\log g(x)dx  \]
\end{itemize}
