\section*{Что это такое?}~\
\\ 

Вспомним всем известную игру <<данетки>>. Так, чтобы понять, о чем идет речь, мы задаем уточняющие вопросы. Так вот, минимальное число вопросов, необходимое, чтобы выяснить полную информацию об объекте, является \textit{энтропией}.

В теории информации энтропия -- \textit{степень неопределенности, связанная со случайной величиной.\footnote{\url{https://stackoverflow.com/questions/510412/what-is-the-computer-science-definition-of-entropy}}}

Также энтропию можно определить как \textit{наименьшее среднее число бит, необходимое для кодирования некоторой информации:}

\[H=-\sum\limits_{i=1}^n p_i\log p_i \]
где $p_i$ --- вероятность $i$-го исхода. Или вероятность того, что в <<данетках>> угадываемый объект обладает некоторой характеристикой. Например, нужно угадать, какого человека загадали, и с вероятностью $2/3$ он моложе 30 лет, с вероятностью $1/3$ старше 30 лет. Такое может быть в ситуации, когда молодых людей среди тех, кого могли бы загадать, больше, или загадывающий отдает предпочтение более молодым людям. 

\subsection*{\hyperref[sec:sol_problem1]{Задача 1.}}\label{sec:problem1} 
Красная Шапочка встретила соседа-лесоруба Николая Петровича по дороге к бабушке Елене, которая равновероятно может жить в одной из трех деревень (Шапка точно помнит, в какой именно). Поскольку девочка маленькая (а неподалеку обитает волк), тот решил узнать, в какой деревне живет бабушка, только не спросив напрямую, а задавая наводящие вопросы. 

Найдите энтропию местонахождения Елены.

\section*{Еще немножко :)}~\
\\

\begin{siderules}
    \textbf{Условная энтропия} --- количество бит, необходимое для того, чтобы узнать имеющуюся информацию о случайной величине $Y$ при условии, что случайная величина $X$ известна.
\end{siderules}

Можно объяснить и проще --- вспомним вновь игру выше. Вам необходимо узнать, кого загадал человек, ведущий в <<данетке>>. Однако теперь он загадывает не одного человека, а \textit{пару}. Каждый человек в этой паре с равными вероятностями может быть как моложе 30 лет (в двух случаях из трех), так и старше 30 (в одном случае из трех). И нам известно, что точно загадали человека моложе 30 лет (одному человеку из этой пары меньше 30 лет). Это и будет наше условие $X$. А далее мы уже исходя из данной информации должны отгадать, кого же все-таки загадали?

Энтропия (среднее число вопросов, необходимое для того, чтобы понять, кого загадали) рассчитывается так:
\[H(Y|X)=-\sum\limits_{x\in X, y\in Y} p(x, y)\log \cfrac{p(x, y)}{p(x)} \]

\subsection*{\hyperref[sec:sol_problem2]{Задача 2.}}\label{sec:problem2} Оказалось, на дороге в одну из трёх деревень (в каждой из которых равновероятно может находиться бабуля Елена) ошивается злой волк Матвей, а в одну деревню ведет только одна дорога. Вероятности того, что Матвей находится в деревне $i$-той ($X$ --- местонахождение волка по вертикали), и того, что Елена в деревне $j$-той ($Y$ --- местонахождение бабули по горизонтали):
\[\begin{pmatrix}
    1/4 & 1/24 & 1/24 \\
    1/24 & 1/4 & 1/24 \\
    1/24 & 1/24 & 1/4 \\
\end{pmatrix} \]

Найдите условную энтропию местонахождения Матвея: $H(X|Y)$. 
\\

\begin{siderules}
    \textbf{Совместная энтропия} --- степень неопределенности, связанная со множеством случайных величин.
\end{siderules}

Как и ранее, ведущий загадал пару людей. Однако теперь мы ничего заранее не знаем, кроме вероятностей, с которыми могли загадать людей, обладающих определенными признаками. Иными словами, вероятность, с которой загадали человека моложе 30, вероятность, с которой волосы загаданного человека имеют рыжий оттенок, и так далее.

Совместная энтропия (среднее число вопросов, необходимое для отгадывания пары людей) рассчитывается так:
\[H(X, Y)=-\sum\limits_{x\in X}\sum\limits_{y\in Y} p(x, y)\log p(x ,y) \]

\subsection*{\hyperref[sec:sol_problem3]{Задача 3.}}\label{sec:problem3} Оказалось, на дороге в одну из трёх деревень (в каждой из которых равновероятно может находиться бабуля Елена) ошивается злой волк Матвей, а в одну деревню ведет только одна дорога. Вероятности того, что Матвей находится в деревне $i$-той ($X$ --- местонахождение волка по вертикали), и того, что Елена в деревне $j$-той ($Y$ --- местонахождение бабули по горизонтали):
\[\begin{pmatrix}
    1/4 & 1/24 & 1/24 \\
    1/24 & 1/4 & 1/24 \\
    1/24 & 1/24 & 1/4 \\
\end{pmatrix} \]

Найдите совместную энтропию местонахождения Елены и Матвея: $H(X, Y)$. \\

\textbf{Все упомянутые выше герои обладают следующими свойствами:}

\begin{itemize}
    \item $H \geqslant 0$
    \item $H(Y|X)=H(X, Y)-H(X)$ или в более общем случае $H(X_1, \ldots, X_n)=\sum\limits_{i=1}^n H(X_i|X_1, \ldots, X_n)$
    \item $H(Y|X)\leqslant H(Y)$
    \item $H(X, Y)=H(X|Y)+H(Y|X)+I(X; Y)=H(X)+H(Y)-I(X; Y)$, где $I(X; Y)$ -- взаимная информация о случайных величинах $X$ и $Y$
    \item $I(X; Y)\leqslant H(X)$, \ $I(X; Y)=H(X)-H(X|Y)$
\end{itemize}

\begin{siderules}
    \textbf{Взаимная информация} $I(X; Y)$ --- мера взаимной зависимости двух случайных величин.
\end{siderules}

Другими словами, \textbf{взаимная информация} --- это те сведения, которые мы получаем, отгадав одного из людей в паре, когда играем в <<данетки>>. Так, чтобы отгадать одного из людей в паре ($X$), в среднем необходимо $H(X)$ вопросов. Чтобы отгадать этого человека, однако зная, что второй загаданный это $Y$, в среднем необходимо $H(X|Y)$ вопросов. Тогда разница в количестве вопросов и есть \textit{взаимная информация}, или те вопросы, которые уже не нужно задавать, если известен один из людей в паре. Также совместная информация обладает свойством \textit{симметричности}: $I(X;Y)=I(Y;X)$. Интуитивно это объясняется тем, что если нам известен один человек из загаданной пары, то часть вопросов (число которых в среднем равно $H(X)$) задавать уже не имеет смысла, поскольку ответ на них следует из уже полученной информации об известном человеке. 

Рассчитывается она так:
\[I(X; Y)=H(X)-H(X|Y)=\]
\[=-\sum\limits_{x\in X} p(x)\log p(x) +\sum\limits_{x\in X, y\in Y} p(x, y)\log \cfrac{p(x, y)}{p(y)} =
\sum\limits_{x\in X}\sum\limits_{y\in Y} p(x, y)\log \cfrac{p(x, y)}{p(x)p(y)} \]

\subsection*{\hyperref[sec:sol_problem4]{Задача 4.}}\label{sec:problem4}  Оказалось, на дороге в одну из трёх деревень (в каждой из которых равновероятно может находиться бравый охотник Борис, защищающий бабулю Елену от диких зверей) ошивается злой волк Матвей, а в одну деревню ведет только одна дорога. Вероятности того, что Матвей находится в деревне $i$-той ($X$ --- местонахождение волка по вертикали), и того, что Борис в деревне $j$-той ($Y$ --- местонахождение бабули по горизонтали):
\[\begin{pmatrix}
    1/5 & 1/15 & 1/15 \\
    1/15 & 1/5 & 1/15 \\
    1/15 & 1/15 & 1/5 \\
\end{pmatrix} \]

Найдите взаимную информацию местонахождения Матвея и Бориса: $I(X; Y)$

\section*{А есть еще кросс энтропия!}~\
\\

\begin{siderules}
    \textbf{Кросс энтропия} --- минимальное среднее количество бит, необходимое для того, чтобы закодировать некоторую информацию, если схема кодирования базируется на некотором распределении $q$, а не истинном, $p$.
\end{siderules}

\[CH(p, q)=-\sum\limits_{i=1}^{n}p_i\log q_i \]

Также кросс энтропию можно определить через \textit{расстояние Кульбака -- Лейблера}. Для начала стоит узнать, что это:

\begin{siderules}
    \textbf{Расстояние Кульбака -- Лейблера} --- степень отдаленности друг от друга двух вероятностных распределений (называется также \textit{относительная энтропия}). \end{siderules}
    
    Интуитивная интерпретация данного понятия такова: насколько больше вопросов мы зададим в случае неоптимальной стратегии игры в <<данетки>>, чем в случае оптимальной стратегии. Можно заметить, что расстояние Кульбака -- Лейблера равно разности кросс-энтропии и энтропии, так как на кодирование информации при неоптимальной стратегии уйдет гораздно больше бит, чем при оптимальной. Мы ищем именно ту разницу, насколько неоптимальная стратегия хуже оптимальной: 
    
    \[D(P\, ||\, Q)= CH(p,q) - H(p), \text{ или } CH(p, q)=H(p)+D_{KL}(p\, || \, q)\]
    
    Рассчитывается для дискретного случая так:

    \[D(P\, ||\, Q)=  - \sum\limits_{i=1}^n p_i\log q_i\ - ( - \sum\limits_{i=1}^n p_i\log p_i)\]



\subsection*{\hyperref[sec:sol_problem5]{Задача 5.}}\label{sec:problem5} Красная Шапочка, убегая от злого лесоруба Николая Петровича, в панике перепутала вероятности, с которыми охотник Борис находится в одной из деревень ($X$ --- местонахождение охотника):
\[\begin{pmatrix}
    1/6 & 2/3 & 1/6
\end{pmatrix} , \] и с которыми волк Матвей ошивается на одной из дорог в деревни ($Y$ --- местонахождение волка):
\[\begin{pmatrix}
    3/8 & 3/8 & 1/4 
\end{pmatrix} .\]

(а) Найдите кросс-энтропию местонахождения Матвея;

(б) Вычислите дивергенцию Кульбака-Лейблера.

\section*{А что, только для дискретных случайных величин?}~\
\\

Нет! :)

В случае, если Вы работаете с абсолютно непрерывными случайными величинами, энтропия и её родственники расчитываются по следующим формулам:
\begin{itemize}
    \item Самая главная и простая энтропийка:
    \[H(X)=-\int\limits_{-\infty}^{+\infty} f(x)\log f(x)dx \]
    \item Условная энтропия:
    \[H(Y|X)=-\int\limits_{-\infty}^{+\infty} f(x, y)\log f_{Y|X}(y)dy \]
    \item Совместная энтропия:
    \[H(X, Y)=-\int\limits_{-\infty}^{+\infty} \int\limits_{-\infty}^{+\infty} f(x, y)\log f(x, y)dxdy \]
    \item Взаимная информация:
    \[I(X; Y)=\int\limits_{-\infty}^{+\infty} \int\limits_{-\infty}^{+\infty} f(x, y) \log \cfrac{f(x, y)}{f(x)f(y)}dxdy \]
    \item Кросс-энтропия:
    \[CH(p, q)=-\int\limits_{-\infty}^{+\infty}p(x)\log q(x) dx \]
    \item Расстояние Кульбака -- Лейблера:
    \[D(P\, ||\, Q)=\int\limits_{-\infty}^{+\infty} f(x)\log f(x)dx -\int\limits_{-\infty}^{+\infty} f(x)\log g(x)dx  \]
\end{itemize}

\subsection*{\hyperref[sec:sol_problem6]{Задача 6.}}\label{sec:problem6} На дороге в деревню, где живет бабушка Елена, равномерно разбросаны монетки. Дорога в эту деревню располагается на отрезке $[0; A]$. Красная Шапочка решила закодировать эту информацию и отправить бабуле с помощью драконей почты. Сколько бит ей понадобится для этого при оптимальной схеме кодирования?

\subsection*{\hyperref[sec:sol_problem7]{Задача 7.}}\label{sec:problem7} Злой лесоруб Николай Петрович решил, что он должен завладеть сердцем Красной Шапки и устранить со своего пути её бабушку Елену, которая против их отношений. Лесоруб не знает, чем конкретно занята сейчас бабушка, но он знает, что она может делать.

Бабушка Елена пробегает от крайней левой деревни до крайней правой (от точки $A=-1$ до точки $B=1$), причём её местоположение отлично описывается следующим распределением:\[f_{\xi}(x)=\begin{cases}
    1-\frac{3}{4}x^2, \ x\in [-1; 1] \\
    0, \text{ иначе}
\end{cases} \]

После того, как Елена пробегает все расстояние, она бежит к кусту (её местоположение равномерно меняется от точки 1 до кустика с ягодами). Если местоположение куста с ягодами является случайной величиной, имеющей равномерное распределение на отрезке $[-1; 1]$, то какова условная энтропия местоположения бабушки?

\subsection*{\hyperref[sec:sol_problem8]{Задача 8.}}\label{sec:problem8} Злой лесоруб Николай Петрович решил, что он должен завладеть сердцем Красной Шапки и устранить со своего пути её бабушку Елену, которая против их отношений. Лесоруб не знает, чем конкретно занята сейчас бабушка, но он знает, что она может делать.

Бабушка Елена пробегает от крайней левой деревни до крайней правой (от точки $A=-1$ до точки $B=1$), причём её местоположение отлично описывается следующим распределением:\[f_{\xi}(x)=\begin{cases}
    1-\frac{3}{4}x^2, \ x\in [-1; 1] \\
    0, \text{ иначе}
\end{cases} \]

После того, как Елена пробегает все расстояние, она бежит к кусту (её местоположение равномерно меняется от точки 1 до кустика с ягодами). Если местоположение куста с ягодами является случайной величиной, имеющей равномерное распределение на отрезке $[-1; 1]$, то какова совместная энтропия местонахождения Елены и куста с ягодами?

\subsection*{\hyperref[sec:sol_problem9]{Задача 9.}}\label{sec:problem9} Злой лесоруб Николай Петрович решил, что он должен завладеть сердцем Красной Шапки и устранить со своего пути её бабушку Елену, которая против их отношений. Лесоруб не знает, чем конкретно занята сейчас бабушка, но он знает, что она может делать.

Бабушка Елена пробегает от крайней левой деревни до крайней правой (от точки $A=-1$ до точки $B=1$), причём её местоположение отлично описывается следующим распределением:\[f_{\xi}(x)=\begin{cases}
    1-\frac{3}{4}x^2, \ x\in [-1; 1] \\
    0, \text{ иначе}
\end{cases} \]

После того, как Елена пробегает все расстояние, она бежит к кусту (её местоположение равномерно меняется от точки 1 до кустика с ягодами). Если местоположение куста с ягодами является случайной величиной, имеющей равномерное распределение на отрезке $[-1; 1]$, то какова взаимная информация местоположения бабушки и куста с ягодами?
 
\subsection*{\hyperref[sec:sol_problem10]{Задача 10.}}\label{sec:problem10} Красная Шапочка должна отнести бабуле пирожки (не только же ягодами ей питаться!). Пока Красная Шапочка бежала к бабушке Елене из её корзинки в какой-то момент начали выпадать пирожки. Всего в корзинке их было $n$ штук. Известно, что пирожки выпадали равномерно на некотором отрезке дороги. Потерю всех пирожков Шапка обнаружила лишь по прибытии к бабушке и сразу решила собрать все выпавшие пирожки. Предполагая, что расстояние от дома Шапочки до дома Бабушки равно $a$, рассчитайте кросс-энтропию, если:

(а) Шапочка знает, что пирожки выпадали равномерно;

(б) Шапочка не знает, что пирожки выпадали равномерно;

(в) Какая из величин больше?

