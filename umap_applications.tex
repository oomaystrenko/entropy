\begin{itemize}
	\item \textbf{Энтропийное кодирование}~\
	
	Как говорилось ранее, энтропия показывает наименьшее среднее число бит, необходимое для кодирования некоторой информации. Данное свойство используется, как ни странно, при кодировании информации.
	
	Например, код Шеннона-Фано. С целью минимизации энтропии и, соответственно, оптимизации кода элементы с большой вероятностью появления кодируются меньшим числом символом. Таким образом, производится сжатие объема информации, что позволяет передавать большее количество информации, затрачивая меньший объем памяти.
	
	\item \textbf{Построение решающих деревьев}
	
	Решающие деревья -- метод, использующийся в машинном обучении и работающий по принципу принятия решений человеком. Каждое ветвление представляет собой разделение выборки на две части по порогу некоторого признака. Например, признак -- длина, пороговое значение --  38. Все объекты, длина которых превышает 38, отделяются от объектов с длиной меньше 38 и дальнейший анализ проходят отдельно.
	\begin{center}
	\tikzstyle{level 1}=[level distance=1cm, sibling distance=5cm]
	\tikzstyle{level 2}=[level distance=1cm, sibling distance=1cm]
	\tikz
	\node {Длина < 38 попугаев}
	child { node {Да}
		child { node {Обыкновенный удав}}}
	child { node {Нет}
		child { node {Анаконда}}};
	\end{center}
	В данном методе расчет энтропии помогает определить оптимальный порог для каждого узла решения. А именно, подбирается такое разделение выборки, при котором сумма энтропий получившихся выборок минимальна среди возможных вариантов разбиений.
	
	Например, у нас есть выборка объектов с одним признаком, длина: 22 попугая (обыкновенный удав), 46 попугаев (анаконда), 40 попугаев (анаконда), 31 попугай (обыкновенный удав). Мы выбираем порог: 38 или 44 попугаев?
	Попробуем разделить выборку по 38 попугаям:
	\begin{center}
	\tikzstyle{level 1}=[level distance=1cm, sibling distance=10cm]
	\tikzstyle{level 2}=[level distance=1cm, sibling distance=1cm]
	\tikz
	\node {Длина < 38 попугаев}
	child { node {Да}
		child { node {Обыкновенный удав, обыкновенный удав}}}
	child { node {Нет}
		child { node {Анаконда, анаконда}}};
	\end{center}
	При расчете энтропии $0 \cdot \log_2 0$ считается равным 0, несмотря на $\log_2 0$. За вероятность принимается вероятность встретить данный класс в новой выборке.
	
	Энтропия левой части: $-(1 \cdot \log_2 1 + 0 \cdot \log_2 0) = 0$.
	
	Энтропия правой части: $-(1 \cdot \log_2 1 + 0 \cdot \log_2 0) = 0$.
	
	Суммарная энтропия получилась: $0$.
	
	Попробуем разделить выборку по 44 попугаям:
	\begin{center}
	\tikzstyle{level 1}=[level distance=1cm, sibling distance=10cm]
	\tikzstyle{level 2}=[level distance=1cm, sibling distance=1cm]
	\tikz
	\node {Длина < 44 попугаев}
	child { node {Да}
		child { node {Обыкновенный удав, анаконда, обыкновенный удав}}}
	child { node {Нет}
		child { node {Анаконда}}};
	\end{center}
	Энтропия левой части: $-(\frac{1}{3} \cdot \log_2 \frac{1}{3} + \frac{2}{3} \cdot \log_2 \frac{2}{3}) \approx 0.92 $.
	
	Энтропия правой части: $-(1 \cdot \log_2 1 + 0 \cdot \log_2 0) = 0$.

	Суммарная энтропия получилась: $0.92$.
	
	В первом случае мы идеально разделили выборку при энтропии, равной нулю. Во втором случае нам удалось отделить одну анаконду, но не удалось отделить классы. Так и энтропия в первом случае оказалась меньше, чем во втором. Причем ее равенство нулю необязательно -- любое значение меньше 0.92 показало бы, что первый случай более оптимален. Здесь же, в виду неотрицательности энтропии, однозначно можно сказать, что критерий "длина < 38 попугаев" дает оптимальный результат.
	
	Энтропия позволяет получать после разбиения выборки, наименее разнообразные по содержанию классов (менее хаотичные). Соответственно, признак и пороговое значение подбираются наиболее оптимально -- алгоритм успешно отделяет объекты, принадлежащие к одному классу.
	
	\item \textbf{Применение в алгоритмах t-SNE и UMAP}
	
	В анализе данных часто возникает необходимость в снижении размерности, и в таких случаях на помощь приходят знания об энтропии. Речь, конечно, идет не об энтропии как таковой, а об алгоритмах, которые базируются на теории.
	
	При создании пространства меньшей размерности, t-SNE и UMAP используют кросс-энтропию как показатель эффективности перенесения свойств объектов. Чем меньше кросс-энтропия, тем ближе к истинному оказалось подобранное распределение.
	
	Приведем пример работы алгоритма UMAP. Мы возьмем набор данных об одежде, который включает в себя 70000 черно-белых изображений различной одежды по 10 классам: футболки, брюки, свитеры, платья, кроссовки и т.д. Каждая картинка имеет размер 28x28 пикселей или 784 пикселя.
	
	Изначально каждый пиксель является признаком объекта (фотографии) и принимает некоторое значение (цвет). Если бы каждая картинка состояла из двух пикселей, мы бы смогли построить график, где по оси абсцисс отложен цвет одного пикселя, по оси ординат цвет второго пикселя, и изобразить точками все объекты.
	
	У нас каждая картинка состоит из 784 пикселей -- 784-мерное пространство, поэтому изобразить его проблематично. Но если мы преобразуем выборку таким образом, что останется всего лишь два признака, то мы сможем визуализировать ее. Получившиеся два признака будут уже не пикселями, а абстрактными признаками, которые алгоритм получает из исходных.
	
	Получить два новых признака можно очень многими способами. Но они должны описывать исходную выборку как можно лучше - чтобы при визуализации мы видели не случайно нарисованное изображение, а отображение начального пространства. Именно тут алгоритм применяет кросс-энтропию. Минимизируя кросс-энтропию между исходным и новым распределением, мы сокращаем отличия между ними, что позволяет получить максимально приближенное отображение данных на плоскости.
	
	Реализуем описанный алгоритм.
	
	Внимание! Библиотека UMAP требует предварительной установки\footnote{Почитать про установку: \url{https://umap-learn.readthedocs.io/en/latest/}}
	
	Импортируем нужные библиотеки:
	\begin{minted}{Python}
import numpy as np # работа с матрицами
from mnist import MNIST # наборы данных
import matplotlib.pyplot as plt # построение графиков
%matplotlib inline
import umap # алгоритм UMAP
	\end{minted}
	
	Загружаем набор данных с фотографиями одежды.
	\begin{minted}{Python}
mndata = MNIST('fashionmnist')
train, train_labels = mndata.load_training() 
test, test_labels = mndata.load_testing()
data = np.array(np.vstack([train, test]), dtype=np.float64) / 255.0
target = np.hstack([train_labels, test_labels])
	\end{minted}

	Cоздаем список из наименований одежды.
\begin{minted}{Python}
	classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
\end{minted}


Запускаем UMAP.

\begin{minted}{Python}
	embedding = umap.UMAP(n_neighbors=10).fit_transform(data)
\end{minted}

	Визуализируем результат.
	\begin{minted}{Python}
plt.figure(figsize=(14, 10))
sns.scatterplot(*embedding.T, hue=target, s=4, palette = 'Spectral',
legend='full', alpha=1.0, edgecolor="none")
plt.legend(classes, loc=1, fontsize='large')
	\end{minted}

Вот что вышло:

\begin{figure}[bh]
	\noindent\centering{
		\includegraphics[height=70mm, width=110mm]{umap.png}
	}
	\caption{Алгоритм UMAP}
	\label{figCurves}
\end{figure}

Это была задача для 784-мерного пространства. И UMAP оказался неким черным ящиком, который перевел его в двумерное пространство непонятным образом. Давайте рассмотрим задачу попроще, чтобы понять, как работает алгоритм.

Пример. В КЭБ717 учатся 3 прекрасные девушки (наши объекты): Даша, Яна и Лиза. Даша -- блондинка с голубыми глазами, у Яны темные волосы и карие глаза, у Лизы русые волосы и карие глаза. Попробуем отобразить объекты в одномерном пространстве, то есть с одним признаком.

Решение. Для начала переведем признаки в числовой вид. 1 признак -- цвет глаз: карие(1), голубые(0). 2 признак -- цвет волос: светлые(0), русые(1), темные(2). Теперь у нас есть три объекта:

\begin{tabular}{|c|c|c|}
	\hline
	 & Цвет глаз & Цвет волос\\
	\hline
	D & 0 & 0\\
	\hline
	Y & 1 & 2\\
	\hline
	L & 1 & 1 \\
	\hline 
\end{tabular}

Теперь найдем расстояния между объектами. По умолчанию, в UMAP стоит евклидова метрика. В нашем случае она также подходит, так как наши признаки могут быть интерпретированы как координаты точек в пространстве.
\end{itemize}