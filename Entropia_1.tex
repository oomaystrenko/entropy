\documentclass[a4paper, 12pt]{scrreprt} 

    \usepackage{cmap}
    \usepackage{graphicx} 
    \usepackage{wrapfig}
    \usepackage{lastpage}
    \usepackage{tikz}
    \usepackage{pgfplots}
    \usepackage{braket}
    \usepackage{verbatim}
    \usetikzlibrary{arrows}
    \usetikzlibrary{calc,positioning,fit,backgrounds}
    \usetikzlibrary{decorations.pathreplacing,calc}
    \usepackage{amsmath} 
    \usepackage{color}
    \usepackage[T2A]{fontenc}
    \usepackage[utf8]{inputenc}	
    \usepackage[english,russian]{babel}
    \usepackage{geometry} 
    \geometry{top=25mm}
    \geometry{bottom=25mm}
    \geometry{left=16mm}
    \geometry{right=16mm}	
    \usepackage{amssymb}
    \usepackage{icomma} 
    \usepackage{mathtext} 
    \usepackage{mathrsfs}
    \usepackage{mathtools}
    \usepackage{fancyhdr}
    \usepackage{mdframed}
    \newmdenv[
  topline=false,
  bottomline=false,
  skipabove=\topsep,
  skipbelow=\topsep,
  leftmargin=20pt,
  rightmargin=20pt,
  innertopmargin=0pt,
  innerbottommargin=0pt
]{siderules}
    \usepackage{hyperref}
    \usepackage{mathtext} 
    \usepackage{multicol}
    \cfoot{\thepage} 
    \hypersetup{
      colorlinks=true
       linkcolor=red,       
            citecolor=black,   
            filecolor=magenta,  
            urlcolor=blue}
    \makeatletter 
    \renewcommand{\headrulewidth}{0,4mm} 
    %ЦВЕТА
    \newcommand{\blue}[1]{\textcolor[rgb]{.4,.2,.4}{#1}}
    \newcommand{\ot}[1]{\textcolor[rgb]{.55,.45,.55}{#1}}
    \newcommand{\oq}[1]{\textcolor[rgb]{.8,.75,.8}{#1}}

    \renewcommand{\maketitle}{\noindent{\bfseries\scshape\LARGE\blue\@title}\par
        \noindent {\large\scshape\bfseries\@subtitle}\par
        \noindent {\slshape\mdseries\oq\@author}
        \vskip 2ex}

    \renewcommand\section{\@startsection{section}{1}{\z@}%
        {-3.5ex \@plus -1ex \@minus -.2ex}%
        {-1em}%
        {\normalfont\large\slshape\bfseries\ot}}
    \makeatother
        
    \renewcommand{\labelitemi}{$\diamond$}

    \author{Марина Аюшеева, Яна Коротова, Олеся Майстренко, Елизавета Махнева, Дарья Писарева}
    \title{Энтропия}

\begin{document}
\maketitle

\section*{Что это такое?}~\

Вспомним всем известную игру "данетки". Так, чтобы понять, о чем идет речь, мы задаем уточняющие вопросы. Так вот, минимальное число вопросов, необходимое, чтобы выяснить полную инофрмацию об объекте, является \textit{энтропией}.

В теории информации энтропия -- \textit{степень неопределенности, связанная со случайной величиной.\footnote{\url{https://stackoverflow.com/questions/510412/what-is-the-computer-science-definition-of-entropy}}}

Также энтропию можно определить как \textit{наименьшее среднее число бит, необходимое для кодирования некоторой информации.}

\[H=-\sum\limits_{i=1}^n p_i\log p_i \]
где $p_i$ -- вероятность $i$-го исхода. Или вероятность того, что в "данетках" угадываемый объект обладает некоторой характеристикой. Например, нужно угадать, какого человека загадали, и с вероятностью $2/3$ он моложе 30 лет, с вероятностью $1/3$ старше 30 лет. Такое может быть в ситуации, когда молодых людей среди тех, кого могли бы загадать, больше, или загадывающий отдает предпочтение более молодым людям. 

\section*{Еще немножко :)}~\

\begin{siderules}
    \textbf{Условная энтропия} --- количество бит, необходимое для того, чтобы закодировать имеющуюся информацию о случайной величине $Y$ при условии, что случайная величина $X$ принимает определенное значение (или просто известна).
\end{siderules}

Можно объяснить и проще -- вспомним вновь игру выше. Вам необходимо узнать, кого загадал человек, ведущий в "данетке". Однако теперь он загадывает не одного человека, а \textit{пару}. Каждый человек в этой паре с равными вероятностями может быть как моложе 30 лет (в 2 случаях из 3), так и старше 30 (в 1 случае из 3). И нам известно, что точно загадали человека моложе 30 лет (одному человеку из этой пары меньше 30 лет). Это и будет наше условие $X$. А далее мы уже исходя из данной информации должны отгадать, кого же все-таки загадали?

Рассчитывается так:
\[H(Y|X)=-\sum\limits_{x\in X, y\in Y} p(x, y)\log \cfrac{p(x, y)}{p(x)} \]

\begin{siderules}
    \textbf{Совместная энтропия} --- степень неопределенности, связанная со множеством случайных величин.
\end{siderules}

Как и ранее, ведущий загадал пару людей. Однако теперь мы ничего заранее не знаем, кроме вероятностей, с которыми могли загадать людей, обладающих определенными признаками. Иными словами, вероятность, с которой загадали человека моложе 30, вероятность, с которой волосы загаданного человека имеют рыжий оттенок, и так далее.

Формула для рассчета:
\[H(X, Y)=-\sum\limits_{x\in X}\sum\limits_{y\in Y} p(x, y)\log p(x ,y) \]

Все упомянутые выше герои обладают следующими свойствами:

\begin{itemize}
    \item $H \geqslant 0$
    \item $H(Y|X)=H(X, Y)-H(X)$ или в более общем случае $H(X_1, \ldots, X_n)=\sum\limits_{i=1}^n H(X_i|X_1, \ldots, X_n)$
    \item $H(Y|X)\leqslant H(Y)$
    \item $H(X, Y)=H(X|Y)+H(Y|X)+I(X; Y)=H(X)+H(Y)-I(X; Y)$, где $I(X; Y)$ -- взаимная информация о случайных величинах $X$ и $Y$
    \item $I(X; Y)\leqslant H(X)$
\end{itemize}

\begin{siderules}
    \textbf{Взаимная информация} --- мера взаимной зависимости двух случайных величин.
\end{siderules}

Другими словами, \textbf{взаимная информация} --- это то, что нам известно о загаданной паре. Если ведущий сначала выбрал одного человека в паре, а затем подобрал второго так, чтобы они как-то были похожи друг на друга или, наоборот, максимально отличались, то информацию о паре можно вычислить, узнав всю информацию о первом человеке в этой паре, затем о втором, сложив их и вычтя те сведения, которые осведомляют о признаках сразу обоих людей.

Рассчитывается она так:
\[I(X; Y)=\sum\limits_{x\in X}\sum\limits_{y\in Y} p(x, y)\log \cfrac{p(x, y)}{p(x)p(y)} \]

\section*{Чуть-чуть истории... }~\
\\

\textbf{В 1948 году}, исследуя проблему рациональной передачи информации через зашумлённый коммуникационный канал, \textbf{Клод Шеннон} предложил революционный вероятностный подход к пониманию коммуникаций и создал первую, истинно математическую, теорию энтропии. 
\\


Его сенсационные идеи быстро послужили основой разработки двух основных направлений: \textit{теории информации}, которая использует понятие вероятности для изучения статистических характеристик данных и коммуникационных систем, и \textit{теории кодирования}, в которой используются главным образом алгебраические и геометрические инструменты для разработки эффективных кодов.
\\


Понятие энтропии, как меры случайности, введено Шенноном в его статье \textit{«Математическая теория связи»} (англ. A Mathematical Theory of Communication), опубликованной в двух частях в Bell System Technical Journal в 1948 году.
\\


В случае равновероятных событий (частный случай), остается зависимость только от количества рассматриваемых вариантов, и формула Шеннона значительно упрощается и совпадает с \textit{формулой Хартли}, которая впервые была предложена американским инженером \textbf{Ральфом Хартли в 1928 году}, как один из научных подходов к оценке сообщений:

\[I=-\log p = \log N ,\]
где $I$ – количество передаваемой информации, $p$ – вероятность события, $N$ – возможное количество различных (равновероятных) сообщений.


 
\section*{А есть еще кросс энтропия!}~\

\begin{siderules}
    \textbf{Кросс энтропия} --- минимальное среднее количество бит, необходимое для того, чтобы закодировать некоторую информацию, если схема кодирования базируется на некотором распределении $q$, а не истинном, $p$.
\end{siderules}

\[CH(p, q)=-\int\limits_{-\infty}^{+\infty}p(x)\log q(x) dx  \]

Также кросс энтропию можно определить через \textit{расстояние Кульбака -- Лейблера}. Для начала стоит узнать, что это:

\begin{siderules}
    \textbf{Расстояние Кульбака -- Лейблера} --- степень отдаленности друг от друга двух вероятностных распределений (называется также \textit{относительная энтропия}). \end{siderules}
    
    Рассчитывается для дискретного случая так:

    \[D(P\, ||\, Q)=\sum\limits_{i=1}^n p_i\log p_i-\sum\limits_{i=1}^n p_i\log q_i\]

    Если имеем дело с абсолютно непрерывными распределениями, тогда формула для расчета выглядит так:
    \[D(P\, ||\, Q)=\int\limits_{-\infty}^{+\infty} f(x)\log f(x)dx -\int\limits_{-\infty}^{+\infty} f(x)\log g(x)dx  \]

Нетрудно заметить, что расстояние Кульбака -- Лейблера равно разности энтропии и кросс-энтропии:

\[D(P\, ||\, Q)=H(p)-CH(p,q),\]
или
\[CH(p, q)=H(p)+D_{KL}(p\, || \, q)\]

\section*{Применение энтропии и ее родственников}~\
\\

\input{umap_applications.tex}


\end{document}
