\documentclass[a4paper, 12pt]{scrreprt} 

    \usepackage{cmap}
    \usepackage{graphicx} 
    \usepackage{wrapfig}
    \usepackage{lastpage}
    \usepackage{tikz}
    \usepackage{pgfplots}
    \usepackage{braket}
    \usepackage{verbatim}
    \usetikzlibrary{arrows}
    \usetikzlibrary{calc,positioning,fit,backgrounds}
    \usetikzlibrary{decorations.pathreplacing,calc}
    \usepackage{amsmath} 
    \usepackage{color}
    \usepackage[T2A]{fontenc}
    \usepackage[utf8]{inputenc}	
    \usepackage[english,russian]{babel}
    \usepackage{geometry} 
    \geometry{top=25mm}
    \geometry{bottom=25mm}
    \geometry{left=16mm}
    \geometry{right=16mm}	
    \usepackage{amssymb}
    \usepackage{icomma} 
    \usepackage{mathtext} 
    \usepackage{mathrsfs}
    \usepackage{mathtools}
    \usepackage{fancyhdr}
    \usepackage{mdframed}
    \newmdenv[
  topline=false,
  bottomline=false,
  skipabove=\topsep,
  skipbelow=\topsep,
  leftmargin=20pt,
  rightmargin=20pt,
  innertopmargin=0pt,
  innerbottommargin=0pt
]{siderules}
    %\pagestyle{fancy}
    %\fancyhf{}
    %\renewcommand{\headrulewidth}{0,04mm}
    \usepackage{hyperref}
    \usepackage{mathtext} 
    \usepackage{multicol}
    %\rhead{\scshape{BEC 171}}
    %\lhead{\scshape{Research seminar for research stream}}
    %\chead{} 
    \cfoot{\thepage} 
    \hypersetup{
      colorlinks=true
       linkcolor=red,       
            citecolor=black,   
            filecolor=magenta,  
            urlcolor=blue}
    \makeatletter 
    \renewcommand{\headrulewidth}{0,4mm} 
    %ЦВЕТА
    \newcommand{\blue}[1]{\textcolor[rgb]{.4,.2,.4}{#1}}
    \newcommand{\ot}[1]{\textcolor[rgb]{.55,.45,.55}{#1}}
    \newcommand{\oq}[1]{\textcolor[rgb]{.8,.75,.8}{#1}}

    \renewcommand{\maketitle}{\noindent{\bfseries\scshape\LARGE\blue\@title}\par
        \noindent {\large\scshape\bfseries\@subtitle}\par
        \noindent {\slshape\mdseries\oq\@author}
        \vskip 2ex}

    \renewcommand\section{\@startsection{section}{1}{\z@}%
        {-3.5ex \@plus -1ex \@minus -.2ex}%
        {-1em}%
        {\normalfont\large\slshape\bfseries\ot}}
    \makeatother
        
    \renewcommand{\labelitemi}{$\diamond$}

    \author{Марина Аюшеева, Яна Коротова, Олеся Майстренко, Елизавета Махнева, Дарья Писарева}
    \title{Энтропия}

\begin{document}
\maketitle  
%\pagestyle{\pagenumber{right}}

\section*{Что это такое?}~\

В теории информации энтропия -- \textit{степень неопределенности, связанная со случайной величиной.\footnote{\url{https://stackoverflow.com/questions/510412/what-is-the-computer-science-definition-of-entropy}}}

Также энтропию можно определить как \textit{наименьшее среднее число бит, необходимое для кодирования некоторой информации.}

\[H=-\sum\limits_{i=1}^n p_i\log p_i \ \text{ или } \ H=-\int\limits_{-\infty}^{+\infty} f(x)\log f(x)dx ,\]
где $f(x)$ -- функция плотности, $p_i$ -- вероятность $i$-го исхода.

\section*{Еще немножко :)}~\

\begin{siderules}
    \textbf{Условная энтропия} --- количество бит, необходимое для того, чтобы закодировать имеющуюся информацию о случайной величине $Y$ при условии, что случайная величина $X$ принимает определенное значение (или просто известна).
\end{siderules}

Рассчитывается так:
\[H(Y|X)=-\sum\limits_{x\in X, y\in Y} p(x, y)\log \cfrac{p(x, y)}{p(x)} \]

\begin{siderules}
    \textbf{Совместная энтропия} --- степень неопределенности, связанная со множеством случайных величин.
\end{siderules}

Формула для рассчета:
\[H(X, Y)=-\sum\limits_{x\in X}\sum\limits_{y\in Y} p(x, y)\log p(x ,y) \]

Все упомянутые выше герои обладают следующими свойствами:

\begin{itemize}
    \item $H \geqslant 0$
    \item $H(Y|X)=H(X, Y)-H(X)$ или в более общем случае $H(X_1, \ldots, X_n)=\sum\limits_{i=1}^n H(X_i|X_1, \ldots, X_n)$
    \item $H(Y|X)\leqslant H(Y)$
    \item $H(X, Y)=H(X|Y)+H(Y|X)+I(X; Y)=H(X)+H(Y)-I(X; Y)$, где $I(X; Y)$ -- взаимная информация о случайных величинах $X$ и $Y$
    \item $I(X; Y)\leqslant H(X)$
\end{itemize}

\begin{siderules}
    \textbf{Взаимная информация} --- мера взаимной зависимости двух случайных величин.
\end{siderules}

Рассчитывается она так:
\[I(X; Y)=\sum\limits_{x\in X}\sum\limits_{y\in Y} p(x, y)\log \cfrac{p(x, y)}{p(x)p(y)} \]

\section*{Чуть-чуть истории... }~\
\\

\textbf{В 1948 году}, исследуя проблему рациональной передачи информации через зашумлённый коммуникационный канал, \textbf{Клод Шеннон} предложил революционный вероятностный подход к пониманию коммуникаций и создал первую, истинно математическую, теорию энтропии. 
\\


Его сенсационные идеи быстро послужили основой разработки двух основных направлений: \textit{теории информации}, которая использует понятие вероятности для изучения статистических характеристик данных и коммуникационных систем, и \textit{теории кодирования}, в которой используются главным образом алгебраические и геометрические инструменты для разработки эффективных кодов.
\\


Понятие энтропии, как меры случайности, введено Шенноном в его статье \textit{«Математическая теория связи»} (англ. A Mathematical Theory of Communication), опубликованной в двух частях в Bell System Technical Journal в 1948 году.
\\


В случае равновероятных событий (частный случай), остается зависимость только от количества рассматриваемых вариантов, и формула Шеннона значительно упрощается и совпадает с \textit{формулой Хартли}, которая впервые была предложена американским инженером \textbf{Ральфом Хартли в 1928 году}, как один из научных подходов к оценке сообщений:

\[I=-\log p = \log N ,\]
где $I$ – количество передаваемой информации, $p$ – вероятность события, $N$ – возможное количество различных (равновероятных) сообщений.


 
\section*{А есть еще кросс энтропия!}~\

\begin{siderules}
    \textbf{Кросс энтропия} --- минимальное среднее количество бит, необходимое для того, чтобы закодировать некоторую информацию, если схема кодирования базируется на некотором распределении $q$, а не истинном, $p$.
\end{siderules}

\[CH(p, q)=-\int\limits_{-\infty}^{+\infty}p(x)\log q(x) dx  \]

Также кросс энтропию можно определить через \textit{расстояние Кульбака -- Лейблера}. Для начала стоит узнать, что это:

\begin{siderules}
    \textbf{Расстояние Кульбака -- Лейблера} --- степень отдаленности друг от друга двух вероятностных распределений (называется также \textit{относительная энтропия}). \end{siderules}
    
    Рассчитывается для дискретного случая так:

    \[D(P\, ||\, Q)=\sum\limits_{i=1}^n p_i\log p_i-\sum\limits_{i=1}^n p_i\log q_i\]

    Если имеем дело с абсолютно непрерывными распределениями, тогда формула для расчета выглядит так:
    \[D(P\, ||\, Q)=\int\limits_{-\infty}^{+\infty} f(x)\log f(x)dx -\int\limits_{-\infty}^{+\infty} f(x)\log g(x)dx  \]

Нетрудно заметить, что расстояние Кульбака -- Лейблера равно разности энтропии и кросс-энтропии:

\[D(P\, ||\, Q)=H(p)-CH(p,q),\]
или
\[CH(p, q)=H(p)+D_{KL}(p\, || \, q)\]

\section*{Применение энтропии и ее родственников}~\
\\

\begin{itemize}
	\item \textbf{Энтропийное кодирование}
	
	Как говорилось ранее, энтропия показывает наименьшее среднее число бит, необходимое для кодирования некоторой информации. Данное свойство используется, как ни странно, при кодировании информации.
	
	Например, код Шеннона-Фано. С целью минимизации энтропии и, соответственно, оптимизации кода элементы с большой вероятностью появления кодируются меньшим числом символом. Таким образом, производится сжатие объема информации, что позволяет передавать большее количество информации, затрачивая меньший объем памяти.
	
	\item \textbf{Построение решающих деревьев}
	
	Решающие деревья - метод, использующийся в машинном обучении и работающий по принципу принятия решений человеком. Каждое ветвление представляет собой разделение выборки на 2 части по порогу некоторого признака. Например, признак - длина, пороговое значение -  2,5. Все объекты, длина которых превышает 2,5, отделяются от объектов с длиной меньше 2,5 и дальнейший анализ проходят отдельно.
	
	В данном методе расчет энтропии помогает определить оптимальный порог для каждого узла решения. А именно, подбирается такое разделение выборки, при котором сумма энтропий получившихся выборок минимальна среди возможных вариантов разбиений.
	
	Это позволяет получать после разбиения выборки, содержащие наименее разнообразные по содержанию классов. Соответственно, признак и пороговое значение подбираются наиболее оптимально - алгоритм успешно отделяет объекты, принадлежащие одному классу.
	
	\item \textbf{Применение в алгоритмах t-SNE и UMAP}
	
	В анализе данных часто возникает необходимость в снижении размерности, и в таких случаях на помощь приходят знания об энтропии, изученной в курсе теории вероятностей. Речь, конечно, идет не об энтропии как таковой, а об алгоритмах, которые базируются на теории.
	
	При создании пространства меньшей размерности, t-SNE и UMAP используют кросс-энтропию как показатель эффективности перенесения свойств объектов. Чем меньше кросс-энтропия, тем ближе к истинному оказалось подобранное распределение.
	
	
\end{itemize}

\input{umap_applications.tex}


\end{document}
